---
title: "Analysis of Household Characteristics in the Jamuna River Basin"
author: "Prabhmeet Kaur"
date: "`r Sys.Date()`"
output: pdf_document
---

## Introduction
  
  In recent years, the anticipatory action sector has seen a significant increase in the number of participants. Despite this growth, there remains a limited amount of literature on targeting interventions, revealing a critical need to explore the characteristics of households benefiting from these programs.

Given that most actors rely on registries to identify their beneficiaries, our project leverages a comprehensive census and baseline data covering 19 unions in the Jamuna River basin. This rich dataset provides an excellent opportunity to analyze and categorize household characteristics by grouping them based on shared attributes.

In this article, we will employ data mining techniques, specifically K-means clustering and Partitioning Around Medoids (PAM), to address the following key questions:
  
  - **Who are the most and least valuable households in this region?**
  - **What are the defining characteristics of these households?**
  
  Through these methods, we aim to deliver actionable insights into household segmentation and value assessment within the studied geography.
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#########################################################################################################
#Setting up
#########################################################################################################

rm(list=ls(all=TRUE))
# Get the username
username <- Sys.getenv("USERNAME")

# Get the user's operating system
os <- Sys.info()["sysname"]


if (os == "Windows") {
  username <- Sys.getenv("USERNAME")
} else if (os == "Darwin"){
  username <- Sys.getenv("USER")
}

# Set the working directory based on username (No Linux, add if someone has a Linux)
# Rocco and Hannah please modify your directories
if (os == "Windows" & username == "econ0683") {
  # Set the working directory for Windows users
  working_dir <- paste("C:/Users/", username,"/Dropbox/Bangladesh Flooding/Data Work/R_ML/", sep="")
}  else if (os == "Darwin" & username == "Prabhmeet") {
  # Set the working directory for macOS users
  working_dir <- paste("/Users/", username, "/Dropbox/Bangladesh Flooding/Data Work/R_ML/", sep="")
} else if (os == "Darwin" & username == "prabhmeetmatta") {
  # Set the working directory for macOS users
  working_dir <- paste("/Users/", username, "/Dropbox/Bangladesh Flooding/Data Work/R_ML/", sep="")
} else {
  # Set a default working directory if the operating system is not recognized
  setwd("~/my_project")
}

# Set the working directory
setwd(working_dir)


# Print the working directory
print(getwd())

# Load and install packages 
installation_needed  <- TRUE
loading_needed <- TRUE

list_packages= c("tidyverse", 'haven', 'lubridate', "tidyr", "dplyr", "ggplot2", "cluster", "factoextra", "readstata13", 'hopkins', 'kableExtra', 'knitr', 'caret', 'NbClust', 'psych', 'labelled')
# Remove the hash if you need to install
#if(installation_needed){install.packages(list_packages, repos='http://cran.us.r-project.org')}
if(loading_needed){lapply(list_packages, require, character.only = TRUE)}
```


```{r clean_data, include=FALSE}
#########################################################################################################
#Harmonising data to be numeric
#########################################################################################################

# Filter out columns with too many missing values (e.g. >20%)
data        <- read.dta13("data_for_ML.dta", convert.factors = TRUE, generate.factors=TRUE, nonint.factors = TRUE) # load data
#Clean data
source(file.path(working_dir, 'cleaning.R'))
```


##Data description
This exercise uses data collected from 14 districts during the census exercise in Bangladesh between April - July 2024. 


##Methodology

#Clustering tendency
We first use the Hopkins statistic to check if the data is clusterable.
The Hopkins statistic (Lawson and Jurs 1990) is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by uniform data distribution.
We use a sample of about 20% of our census database for the analysis. As the Hopkins statistic is close to 1, this indicates that the data is highly clustered.

```{r cluster_tendency_1, echo=FALSE}
#########################################################################################################
#Checking for clustering tendency
#########################################################################################################

# Clustering tendency with hopkins
kable(describe(hh_filtered, fast = TRUE),format = 'html') %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

## Clustering Analysis
#The non-parametric analysis below using PAM and K-means: Partitioning Around Medoids to discover similarities between households in the baseline dataset. PAM instead of the go to K-means clustering is used, given the prior of the outliers and substantial variance in many variables of interests, e.g. income. 

### Silhouette Analysis for optimal clusters 
#Some optimality analysis for # of clusters: a.k.a. silhouette
#Give the size of the dataset, sample only 10% of these households
set.seed(42)
sample_pct = 0.2
hh_sampled <- hh_filtered %>% sample_frac(sample_pct)
# Remove rows with NA or Infinite values
hh_sampled <- hh_sampled[complete.cases(hh_sampled), ]
hh_sampled <- hh_sampled[!is.infinite(rowSums(hh_sampled)), ]
hh_sampled <- na.omit(hh_sampled)

# Checking for columns with near zero variance 
nzv <- nearZeroVar(hh_sampled) 
#removing these
```
## Data
This exercise uses data collected during a census exercise in 14 unions in Bangladesh in 2024. The data consists of ***** households located in the Jamuna River Basin. We use a sample of about 20% of our census database in order to investigate clusterability. Variables used for the clustering analysis are listed below. 

```{r summary, echo=FALSE}
#########################################################################################################
#Summary table
#########################################################################################################
kable(describe(hh_sampled, range=FALSE, skew = FALSE),format = 'html',  digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


## Methodology
# Clustering tendency
The Hopkins statistic is computed to check if the data is clusterable. As the Hopkins statistic is close to 1, this indicates that the data is highly clustered
```{r cluster_tend}

# Hopkins Statistic
hh_sampled <- hh_sampled[, -nzv] #removing these
h <- hopkins::hopkins(hh_sampled) # statistic =1 so from the documentation we know that the data is highly clustered (Calculated values 0-0.3 indicate regularly-spaced data. Values around 0.5 indicate random data. Values 0.7-1 indicate clustered data.)
stat <- unlist(h)
h <- 1-stat
print(paste0("The hopkins statistic is", " ", h))
```

# Choosing optimal number of clusters
We calculate 9 indices for determining the number of clusters, using ward method and euclidean distance. 

```{r num_clusters, echo=FALSE}
# Plotting The Visual Assessment of cluster tendency (VAT) to visually determine if there are any potenial clusterings
factoextra::get_clust_tendency(data = hh_sampled, n = 10)
NbClust(hh_sampled, distance="euclidean", min.nc=2, max.nc=10, method="ward.D2", index="dindex") 
NbClust(hh_sampled, distance="manhattan", min.nc=2, max.nc=10, method="ward.D2", index="dindex") 
# In the plot of D index, a significant knee seems to be for 4 clusters. Therefore the optimal number of clusters according to D index is 4.

# List of all clustering indices from your documentation
#"ccc", "scott", "marriot", "trcovw", "tracew", "friedman", "rubin", "ptbiserial", "gamma", "gplus", "tau", "dunn", "hubert", "sdindex", "dindex", "sdbw", gap "duda", "pseudot2", "beale",
clustering_indices <- c(
  "kl", "ch", "hartigan",  "cindex", 
  "db", "silhouette",
  "ratkowsky", "ball","frey"
)

clustering_names <- c(
  "Krzanowski and Lai (1988)", "Calinski and Harabasz (1974)", "Hartigan (1975)",  "Hubert and Levin (1976)",
  "Davies and Bouldin (1979)", "Rousseeuw (1987)",
  "Ratkowsky and Lance (1978)", "Ball and Hall (1965)","Frey and Van Groenewoud (1972)"
)

# Initialize an empty list to store results
num_clusters_list <- list()
  
# Iterate over each index and run NbClust
for (index in clustering_indices) {
  cat("Processing index:", index, "\n")  # Optional: print to show progress
  
  # Wrap in tryCatch to handle potential errors with specific indices
  tryCatch({
    res <- NbClust(hh_sampled, diss = NULL, distance = "euclidean", min.nc = 2, max.nc = 10, 
                   method = "ward.D2", index = index)
    
    # Extract the optimal number of clusters and store in the list
    num_clusters_list[[index]] <- res$Best.nc[1]
  }, error = function(e) {
    # If there's an error, store NA for this index and print an error message
    cat("Error with index:", index, " - ", e$message, "\n")
    num_clusters_list[[index]] <- NA
  })
}

# Convert the list to a data frame for better display
results <- data.frame(
  Method = clustering_names,  # Method names will be the same as the index names
  Optimal_Clusters = unlist(num_clusters_list)
)

# Display the results
print(results)
kable(results, format='html') %>%
 kable_styling(bootstrap_options = c("striped", "hover"))

# Bar chart with optimal number of clusters
barplot(table(results[,"Optimal_Clusters"]),
        col=c("grey"), 
        main="Optimal number of clusters", 
        xlab="Number of clusters",
      ylab="Frequency among all indices")

```
From the bar graph we see that the the indices state that the data can be sub-divided into 2 or 3 optimal number of clusters. We now move towards silhouette analysis for clustering.

##Silhouette Analysis

Silhouette Analysis can help us measure the coherence of clusters. It evaluates the quality of clusters in data by measuring how similar each observation is to its assigned cluster compared to other clusters

The **Silhouette Coefficient** \(s(i)\) is:

$$
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
$$

Where:
- \(a(i)\) is the average distance between the point \(i\) and other points in the same cluster.
- \(b(i)\) is the minimum of average distance between the point \(i\) and points in another cluster.


The coefficient has a range of [-1, 1]. Values close to 1 indicate that the cluster is coherent, while values close to -1 indicate that the values might be assigned to the wrong cluster.


## K-means and PAM

K-means aims to partition data into \( K \) clusters by identifying \( K \) centroids, which are representative points that act as the center of each cluster. These centroids may not correspond to actual data points; rather, they are average positions calculated based on the points in each cluster.

In contrast, Partitioning Around Medoids (PAM) uses **medoids** instead of centroids. Medoids are always actual data points within the dataset, chosen to represent the central point of a cluster. This makes PAM more robust to noise and outliers compared to K-means, as medoids are less influenced by extreme values. We will use both K-means and PAM to discover similarities between households in the baseline dataset and rely on the silhouette width metric to ascertain which method fits the data better.


```{r silhouette}
### Clustering with PAM
fviz_nbclust(hh_sampled, pam, method ="silhouette")+theme_minimal() # 2

km_pam <- eclust(hh_sampled,FUNcluster="pam", k=10,hc_metric = "euclidean")
km.sil_pam<-silhouette(km_pam$cluster, dist(hh_sampled))
fviz_silhouette(km.sil_pam)

fviz_nbclust(hh_sampled, kmeans, method="silhouette")+theme_classic() # 2
km <- eclust(hh_sampled,FUNcluster="kmeans", k=2,hc_metric = "euclidean")
km.sil<-silhouette(km$cluster, dist(hh_sampled))
fviz_silhouette(km.sil)


```


For PAM we see that the optimal number of clusters is 2 giving us an average silhouette width of about 0.54, while for K-means the optimal number of clusters is 2 with an average silhouette width of .78. As the average silhouette width with k-means is higher, K-means seems to fit the data better, so we continue the analysis with K-means.

## Results
We visualize the primary outcomes for the two clusters to discern differences if any.

``` {r subgroup_plot, echo=FALSE}
hh_sampled.c<-cbind(hh_sampled, km$cluster)
colnames(hh_sampled.c)[103]<-c("Group")

df.m <- melt(hh_sampled.c, id.var = "Group")
df.m$Group <- as.character(df.m$Group)

# Define the variables you want to include in your plot
variables_to_plot <- c("FCS", "stair_numeric", "land_own", "monthly_income", "count_assets_hhdurable", "ppi") 

# Filter the melted data frame to include only these variables
df.m <- df.m %>% filter(variable %in% variables_to_plot)


p <- ggplot(data = df.m, aes(x=variable, y=value)) +
  geom_boxplot(aes(fill = Group),outlier.size = 1) +
  facet_wrap( ~ variable, scales="free", ncol = 2) +
  xlab(label = NULL) + ylab(label = NULL) + ggtitle("Boxplots for 2 Groups of Households") +
  guides(fill=guide_legend(title="Groups"))

p 

```
The box plots reveal distinct segments within the population, identifying an elite group and a non-elite group through clustering analysis. The elite group demonstrates significantly higher levels of asset holdings, food consumption scores, land ownership, and average monthly incomes compared to the non-elite group. Below, we present summary statistics for both groups, providing a clearer comparison of their socioeconomic characteristics.

# Descriptives by group (Non-elite)

```{r tables}

# Selecting specific columns for the table
group <- hh_sampled.c %>% select(hh_size, FCS, stair_numeric, land_own, monthly_income, count_assets_hhdurable, electricity, count_asset_loss)



GroupsSummary <- describeBy(group,hh_sampled.c[,'Group'])

kable(GroupsSummary[[1]], format = "html", digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

# Descriptives by group (Elite)

```{r}
kable(GroupsSummary[[2]], format = "html", digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Conclusion
The clustering exercise provided valuable insights into the population's structure, enabling us to identify and segment elite individuals. This analysis revealed that the elite population possesses significant advantages over their non-elite counterparts, characterized by higher asset holdings, improved food consumption scores, greater land ownership, and elevated average monthly incomes. Understanding these distinctions is crucial for informing targeted interventions and policies aimed at the most vulnerable within the population.
